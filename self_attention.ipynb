{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class Self_Attention(torch.nn.Module):\n",
    "    # input : batch_size * seq_len * input_dim\n",
    "    # q : batch_size * input_dim * dim_k\n",
    "    # k : batch_size * input_dim * dim_k\n",
    "    # v : batch_size * input_dim * dim_v\n",
    "    def __init__(self,input_dim,dim_k,dim_v):\n",
    "        super(Self_Attention,self).__init__()\n",
    "        self.q = torch.nn.Linear(input_dim,dim_k)\n",
    "        self.k = torch.nn.Linear(input_dim,dim_k)\n",
    "        self.v = torch.nn.Linear(input_dim,dim_v)\n",
    "        self._norm_fact = 1 / sqrt(dim_k)\n",
    "        \n",
    "    \n",
    "    def forward(self,x):\n",
    "        Q = self.q(x) # Q: batch_size * seq_len * dim_k\n",
    "        K = self.k(x) # K: batch_size * seq_len * dim_k\n",
    "        V = self.v(x) # V: batch_size * seq_len * dim_v\n",
    "        print('before: ', torch.nn.Softmax(dim=1)(torch.bmm(Q,K.permute(0,2,1))) )\n",
    "        atten = torch.nn.Softmax(dim=1)(torch.bmm(Q,K.permute(0,2,1))) * self._norm_fact # Q * K.T() # batch_size * seq_len * seq_len\n",
    "        print('atten: ', atten)\n",
    "        output = torch.bmm(atten,V) # Q * K.T() * V # batch_size * seq_len * dim_v\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before:  tensor([[[0.3070, 0.3090, 0.2706],\n",
      "         [0.3013, 0.3037, 0.2582],\n",
      "         [0.3917, 0.3874, 0.4712]],\n",
      "\n",
      "        [[0.3149, 0.2928, 0.2381],\n",
      "         [0.3272, 0.3274, 0.3223],\n",
      "         [0.3580, 0.3798, 0.4395]],\n",
      "\n",
      "        [[0.3581, 0.3378, 0.3725],\n",
      "         [0.2882, 0.3121, 0.2705],\n",
      "         [0.3537, 0.3502, 0.3570]],\n",
      "\n",
      "        [[0.3610, 0.3407, 0.3819],\n",
      "         [0.2546, 0.2936, 0.2106],\n",
      "         [0.3845, 0.3657, 0.4075]]], grad_fn=<SoftmaxBackward>)\n",
      "atten:  tensor([[[0.1535, 0.1545, 0.1353],\n",
      "         [0.1506, 0.1518, 0.1291],\n",
      "         [0.1958, 0.1937, 0.2356]],\n",
      "\n",
      "        [[0.1574, 0.1464, 0.1191],\n",
      "         [0.1636, 0.1637, 0.1612],\n",
      "         [0.1790, 0.1899, 0.2198]],\n",
      "\n",
      "        [[0.1790, 0.1689, 0.1862],\n",
      "         [0.1441, 0.1560, 0.1353],\n",
      "         [0.1769, 0.1751, 0.1785]],\n",
      "\n",
      "        [[0.1805, 0.1703, 0.1910],\n",
      "         [0.1273, 0.1468, 0.1053],\n",
      "         [0.1922, 0.1829, 0.2038]]], grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 5])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand((4,3,2))\n",
    "model = Self_Attention(2, 4, 5)\n",
    "res = model(x)\n",
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5801, 0.4199],\n",
       "         [0.3173, 0.6827]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand((1,2,2))\n",
    "torch.nn.Softmax(dim=-1)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.sum([[0.3323, 0.1323, 0.5354],\n",
    "        [0.1261, 0.4239, 0.4500]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3323, 0.1323, 0.5354],\n",
       "        [0.1261, 0.4239, 0.4500]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = nn.Softmax(dim=-1)\n",
    "input = torch.randn(2, 3)\n",
    "output = m(input)\n",
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
